# ğŸ“° BizNews - Sistema de Scraping y VisualizaciÃ³n de Noticias

Un sistema completo de extracciÃ³n, almacenamiento y visualizaciÃ³n de noticias de mÃºltiples fuentes peruanas, con dashboard interactivo y reportes estadÃ­sticos.

## ğŸ—ï¸ Arquitectura del Proyecto

```
news_scraping/
â”œâ”€â”€ backend/                    # Backend (FastAPI + PostgreSQL)
â”‚   â””â”€â”€ scraping-project/
â”‚       â”œâ”€â”€ api/               # API REST con FastAPI
â”‚       â”œâ”€â”€ scraper/           # Spiders de Scrapy
â”‚       â”œâ”€â”€ pipelines/         # Procesamiento de datos
â”‚       â””â”€â”€ celery_tasks/      # Tareas programadas
â”œâ”€â”€ frontend/                  # Frontend (HTML + JavaScript)
â”‚   â””â”€â”€ biznews/              # Interfaz web responsive
â””â”€â”€ data/                     # Datos extraÃ­dos (CSV/JSON)
```

## ğŸš€ Inicio RÃ¡pido

### Prerrequisitos

- **Python 3.8+**
- **PostgreSQL 12+**
- **Redis** (opcional, para tareas programadas)
- **Git**

### 1. Clonar el Repositorio

```bash
git clone <repository-url>
cd news_scraping
```

### 2. Configurar Backend

```bash
# Navegar al directorio del backend
cd scraping-project

# Crear entorno virtual
python -m venv venv

# Activar entorno virtual
# Windows:
venv\Scripts\activate
# Linux/Mac:
source venv/bin/activate

# Instalar dependencias
pip install -r api/requirements.txt
```

### 3. Configurar Base de Datos

```bash
# Crear base de datos PostgreSQL
createdb news_db

# Configurar variables de entorno
cp env_example.txt .env
# Editar .env con tus credenciales de PostgreSQL
```

### 4. Inicializar Base de Datos

```bash
# Ejecutar migraciones
python setup_database.py

# Migrar datos existentes (opcional)
python migrate_to_postgres.py
```

### 5. Iniciar Backend

```bash
# Iniciar API FastAPI
uvicorn api.main:app --reload --host 127.0.0.1 --port 8000
```

### 6. Iniciar Frontend

```bash
# En otra terminal, navegar al frontend
cd biznews

# Iniciar servidor HTTP local
python serve.py
# O usar el script batch en Windows
start_server.bat
```

### 7. Acceder a la AplicaciÃ³n

- **Frontend**: http://localhost:8080
- **API**: http://localhost:8000
- **DocumentaciÃ³n API**: http://localhost:8000/docs

## ğŸ“Š Fuentes de Noticias

El sistema extrae noticias de las siguientes fuentes:

- **Pachamama Radio** - Noticias regionales de Puno
- **Puno Noticias** - InformaciÃ³n local de Puno
- **Los Andes** - PeriÃ³dico regional
- **Sin Fronteras** - Medios alternativos

## ğŸ¯ Funcionalidades

### Frontend (BizNews)

#### ğŸ  PÃ¡gina Principal
- **Portada**: Noticias destacadas y Ãºltimas noticias
- **Noticias de Puno**: SecciÃ³n especializada con filtros
- **Noticias Trending**: MÃ¡s populares y relevantes
- **NavegaciÃ³n**: MenÃº intuitivo entre secciones

#### ğŸ“° GestiÃ³n de Contenido
- **Fuentes**: VisualizaciÃ³n por fuente de noticias
- **CategorÃ­as**: OrganizaciÃ³n por temas
- **Filtros de Tiempo**: Hoy, semana, mes, aÃ±o
- **Detalle de Noticias**: PÃ¡ginas individuales con contenido completo

#### ğŸ“ˆ Reportes y EstadÃ­sticas
- **GrÃ¡fica de Fuentes**: DistribuciÃ³n por fuente (Dona)
- **GrÃ¡fica de CategorÃ­as**: Noticias por categorÃ­a (Barras)
- **Tendencia Mensual**: EvoluciÃ³n temporal (LÃ­nea)
- **DÃ­as de la Semana**: Patrones de publicaciÃ³n (Polar)
- **Top Noticias**: Ranking de popularidad (Barras horizontales)
- **Resumen EstadÃ­stico**: MÃ©tricas clave del sistema

### Backend (API)

#### ğŸ”Œ Endpoints Principales

```http
# Noticias
GET /news                    # Listar todas las noticias
GET /news/{id}              # Obtener noticia especÃ­fica
GET /news/fuentes/{fuente}  # Noticias por fuente
GET /news/categorias/{cat}  # Noticias por categorÃ­a

# Metadatos
GET /news/fuentes/listar    # Listar fuentes disponibles
GET /news/categorias/listar # Listar categorÃ­as disponibles

# Filtros
GET /news?fecha_desde=YYYY-MM-DD  # Filtrar por fecha
GET /news?limit=50&skip=0        # PaginaciÃ³n
```

#### ğŸ•·ï¸ Sistema de Scraping

- **Scrapy Spiders**: ExtracciÃ³n automatizada de noticias
- **Pipelines**: Limpieza y procesamiento de datos
- **Celery Tasks**: ProgramaciÃ³n de extracciones
- **DeduplicaciÃ³n**: Evita noticias duplicadas

## ğŸ› ï¸ ConfiguraciÃ³n Avanzada

### Variables de Entorno

Crear archivo `.env` en `scraping-project/`:

```env
# Base de datos
DB_HOST=localhost
DB_PORT=5432
DB_NAME=news_db
DB_USER=postgres
DB_PASSWORD=tu_password

# Redis (opcional)
REDIS_URL=redis://localhost:6379

# CORS
FRONTEND_ORIGINS=http://localhost:8080,http://127.0.0.1:8080
```

### ConfiguraciÃ³n de Scraping

```bash
# Ejecutar scraping manual
cd scraping-project
scrapy crawl pachamamaradio
scrapy crawl punonoticias
scrapy crawl losandes
scrapy crawl sinfronteras

# Ejecutar todos los spiders
python run_all_spiders_complete.py
```

### Tareas Programadas (Celery)

```bash
# Iniciar worker de Celery
python celery_workers/start_worker.py

# Iniciar scheduler
python celery_workers/start_beat.py
```

## ğŸ“ Estructura Detallada

### Backend (`scraping-project/`)

```
scraping-project/
â”œâ”€â”€ api/                      # API FastAPI
â”‚   â”œâ”€â”€ main.py              # AplicaciÃ³n principal
â”‚   â”œâ”€â”€ models.py            # Modelos de datos
â”‚   â”œâ”€â”€ db.py                # ConexiÃ³n a base de datos
â”‚   â””â”€â”€ routers/             # Endpoints de la API
â”‚       â””â”€â”€ news.py          # Endpoints de noticias
â”œâ”€â”€ scraper/                 # Spiders de Scrapy
â”‚   â”œâ”€â”€ settings.py          # ConfiguraciÃ³n de Scrapy
â”‚   â””â”€â”€ spiders/             # Spiders individuales
â”œâ”€â”€ pipelines/               # Procesamiento de datos
â”‚   â”œâ”€â”€ clean_pipeline.py    # Limpieza de contenido
â”‚   â””â”€â”€ postgres_pipeline.py # Almacenamiento en BD
â”œâ”€â”€ celery_tasks/            # Tareas programadas
â”‚   â”œâ”€â”€ scraping_tasks.py    # Tareas de scraping
â”‚   â””â”€â”€ cleanup_tasks.py     # Tareas de limpieza
â””â”€â”€ data/                    # Datos extraÃ­dos
    â”œâ”€â”€ pachamamaradio/
    â”œâ”€â”€ punonoticias/
    â”œâ”€â”€ losandes/
    â””â”€â”€ sinfronteras/
```

### Frontend (`biznews/`)

```
biznews/
â”œâ”€â”€ index.html               # PÃ¡gina principal
â”œâ”€â”€ fuentes.html             # PÃ¡gina de fuentes
â”œâ”€â”€ categorias.html          # PÃ¡gina de categorÃ­as
â”œâ”€â”€ reportes.html            # Dashboard de estadÃ­sticas
â”œâ”€â”€ detalle_noticias.html    # PÃ¡gina de noticia individual
â”œâ”€â”€ contact.html             # PÃ¡gina de contacto
â”œâ”€â”€ js/                      # JavaScript
â”‚   â”œâ”€â”€ news-api.js          # Cliente de API
â”‚   â”œâ”€â”€ fuentes.js           # LÃ³gica de fuentes
â”‚   â”œâ”€â”€ categorias.js        # LÃ³gica de categorÃ­as
â”‚   â””â”€â”€ reportes.js          # GrÃ¡ficas y estadÃ­sticas
â”œâ”€â”€ css/                     # Estilos
â”‚   â”œâ”€â”€ style.css            # Estilos principales
â”‚   â”œâ”€â”€ custom-fixes.css     # Correcciones personalizadas
â”‚   â””â”€â”€ reportes.css         # Estilos de reportes
â”œâ”€â”€ img/                     # ImÃ¡genes
â””â”€â”€ serve.py                 # Servidor HTTP local
```

## ğŸ”§ TecnologÃ­as Utilizadas

### Backend
- **FastAPI**: Framework web moderno y rÃ¡pido
- **PostgreSQL**: Base de datos relacional
- **Scrapy**: Framework de web scraping
- **Celery**: Tareas asÃ­ncronas programadas
- **Redis**: Broker de mensajes (opcional)
- **SQLAlchemy**: ORM para base de datos
- **Pydantic**: ValidaciÃ³n de datos

### Frontend
- **HTML5**: Estructura semÃ¡ntica
- **CSS3**: Estilos responsive
- **JavaScript ES6+**: LÃ³gica del cliente
- **Chart.js**: GrÃ¡ficas interactivas
- **Bootstrap 4**: Framework CSS
- **Font Awesome**: IconografÃ­a

### Herramientas
- **Uvicorn**: Servidor ASGI
- **Python HTTP Server**: Servidor de archivos estÃ¡ticos
- **Git**: Control de versiones

## ğŸ“Š Base de Datos

### Esquema Principal

```sql
-- Tabla de noticias
CREATE TABLE noticias (
    id SERIAL PRIMARY KEY,
    titulo VARCHAR(500) NOT NULL,
    fecha DATE,
    hora TIME,
    resumen TEXT,
    contenido TEXT,
    categoria VARCHAR(100),
    autor VARCHAR(200),
    tags TEXT,
    url VARCHAR(500) UNIQUE,
    fecha_extraccion TIMESTAMP,
    imagenes TEXT,
    fuente VARCHAR(100),
    created_at TIMESTAMP DEFAULT NOW()
);
```

### Ãndices Optimizados

```sql
CREATE INDEX idx_fuente ON noticias(fuente);
CREATE INDEX idx_categoria ON noticias(categoria);
CREATE INDEX idx_fecha ON noticias(fecha);
CREATE INDEX idx_url ON noticias(url);
```

## ğŸš€ Despliegue

### Desarrollo Local

```bash
# Terminal 1: Backend
cd scraping-project
venv\Scripts\activate
uvicorn api.main:app --reload --host 127.0.0.1 --port 8000

# Terminal 2: Frontend
cd biznews
python serve.py
```

### ProducciÃ³n

```bash
# Usar Gunicorn para producciÃ³n
pip install gunicorn
gunicorn api.main:app -w 4 -k uvicorn.workers.UvicornWorker

# Servir frontend con Nginx
# Configurar proxy reverso para API
```

## ğŸ› SoluciÃ³n de Problemas

### Error de CORS
```bash
# Verificar que FRONTEND_ORIGINS estÃ© configurado
# En api/main.py se incluyen orÃ­genes comunes
```

### Error de Base de Datos
```bash
# Verificar conexiÃ³n a PostgreSQL
python diagnose_postgres.py

# Recrear base de datos
dropdb news_db
createdb news_db
python setup_database.py
```

### Error de Scraping
```bash
# Verificar conectividad
python test_scraping.py

# Ejecutar spider individual
scrapy crawl pachamamaradio -L INFO
```

## ğŸ“ˆ Monitoreo

### Logs del Sistema
```bash
# Logs de API
tail -f logs/api.log

# Logs de Scraping
tail -f logs/scraping.log

# Logs de Celery
tail -f logs/celery.log
```

### MÃ©tricas de Rendimiento
- **Tiempo de respuesta API**: < 200ms
- **Tasa de Ã©xito scraping**: > 95%
- **Uso de memoria**: < 512MB
- **Almacenamiento**: ~1GB por 10,000 noticias

## ğŸ¤ ContribuciÃ³n

1. Fork el proyecto
2. Crear rama para feature (`git checkout -b feature/nueva-funcionalidad`)
3. Commit cambios (`git commit -m 'Agregar nueva funcionalidad'`)
4. Push a la rama (`git push origin feature/nueva-funcionalidad`)
5. Crear Pull Request

## ğŸ“ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT. Ver `LICENSE` para mÃ¡s detalles.

## ğŸ‘¥ Autores

- **Desarrollador Principal**: [Tu Nombre]
- **MinerÃ­a de Datos**: Ciclo 8
- **Universidad**: [Nombre de la Universidad]

## ğŸ“ Soporte

Para soporte tÃ©cnico o preguntas:
- **Email**: [tu-email@ejemplo.com]
- **Issues**: [GitHub Issues]
- **DocumentaciÃ³n**: [Wiki del Proyecto]

---

## ğŸ¯ Roadmap

### VersiÃ³n 2.0
- [ ] Sistema de usuarios y autenticaciÃ³n
- [ ] Notificaciones push
- [ ] API de bÃºsqueda avanzada
- [ ] ExportaciÃ³n de datos (PDF, Excel)
- [ ] Dashboard de administraciÃ³n

### VersiÃ³n 3.0
- [ ] Machine Learning para categorizaciÃ³n
- [ ] AnÃ¡lisis de sentimientos
- [ ] DetecciÃ³n de noticias duplicadas
- [ ] IntegraciÃ³n con redes sociales
- [ ] AplicaciÃ³n mÃ³vil

---

**Â¡Gracias por usar BizNews! ğŸš€**
