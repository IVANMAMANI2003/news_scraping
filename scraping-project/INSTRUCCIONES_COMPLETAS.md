# ğŸ“° Sistema Completo de Scraping de Noticias con PostgreSQL

## ğŸ¯ Resumen del Proyecto

Has creado un sistema completo de scraping de noticias que:
- âœ… Extrae noticias de mÃºltiples fuentes
- âœ… Almacena datos en PostgreSQL automÃ¡ticamente
- âœ… Detecta duplicados por URL
- âœ… Se ejecuta de forma programada
- âœ… Mantiene logs detallados

## ğŸ“ Archivos Creados

### ğŸ”§ ConfiguraciÃ³n y Base de Datos
- `setup_database.py` - Configura PostgreSQL y crea la tabla
- `config/database.py` - ConfiguraciÃ³n de la base de datos
- `env_example.txt` - Variables de entorno (copia a .env)

### ğŸ•·ï¸ Pipelines de Scrapy
- `pepelines/clean_pipeline.py` - Limpieza de datos (ya existÃ­a)
- `pepelines/postgres_pipeline.py` - **NUEVO** - Guarda en PostgreSQL

### ğŸ•·ï¸ Spiders de Scrapy
- `spiders/losandes_scrapy_spider.py` - **NUEVO** - Spider para Los Andes
- `spiders/pachamamaradio_spider.py` - Spider existente (de Colab)
- `spiders/punonoticias_spider.py` - Spider existente (de Colab)
- `spiders/sinfronteras_spider.py` - Spider existente (de Colab)

### ğŸš€ Scripts de EjecuciÃ³n
- `run_scraping.py` - **NUEVO** - Ejecuta todos los spiders
- `scheduler.py` - **NUEVO** - Programador automÃ¡tico
- `test_scraping.py` - **NUEVO** - Pruebas del sistema
- `install_and_setup.py` - **NUEVO** - InstalaciÃ³n automÃ¡tica

### ğŸ”„ Utilidades
- `convert_colab_to_scrapy.py` - **NUEVO** - Convierte spiders de Colab
- `README.md` - **NUEVO** - DocumentaciÃ³n completa

## ğŸš€ Instrucciones de InstalaciÃ³n

### 1. Instalar Dependencias
```bash
pip install -r requeriments.txt
```

### 2. Configurar PostgreSQL
```bash
# Crear base de datos y tabla
python setup_database.py
```

### 3. Configurar Variables de Entorno
```bash
# Copiar archivo de ejemplo
cp env_example.txt .env

# Editar con tus credenciales
# DB_PASSWORD=tu_contraseÃ±a_de_postgresql
```

### 4. InstalaciÃ³n AutomÃ¡tica (Recomendado)
```bash
python install_and_setup.py
```

## ğŸ® CÃ³mo Usar el Sistema

### EjecuciÃ³n Manual
```bash
# Ejecutar todos los spiders
python run_scraping.py

# Ejecutar un spider especÃ­fico
scrapy crawl losandes
scrapy crawl pachamamaradio
scrapy crawl punonoticias
scrapy crawl sinfronteras
```

### EjecuciÃ³n AutomÃ¡tica
```bash
# Programador que ejecuta cada 6 horas
python scheduler.py
```

### Pruebas
```bash
# Verificar que todo funciona
python test_scraping.py
```

## ğŸ—„ï¸ Estructura de la Base de Datos

La tabla `noticias` tiene estos campos:

| Campo | Tipo | DescripciÃ³n |
|-------|------|-------------|
| id | SERIAL | Clave primaria |
| titulo | TEXT | TÃ­tulo de la noticia |
| fecha | TIMESTAMP | Fecha de publicaciÃ³n |
| resumen | TEXT | Resumen de la noticia |
| contenido | TEXT | Contenido completo |
| categoria | VARCHAR(100) | CategorÃ­a |
| autor | VARCHAR(200) | Autor |
| tags | TEXT | Tags separados por comas |
| url | TEXT UNIQUE | URL Ãºnica (evita duplicados) |
| fecha_extraccion | TIMESTAMP | CuÃ¡ndo se extrajo |
| caracteres_contenido | INTEGER | NÃºmero de caracteres |
| palabras_contenido | INTEGER | NÃºmero de palabras |
| imagenes | TEXT | URLs de imÃ¡genes (JSON) |
| fuente | VARCHAR(100) | Fuente de la noticia |
| created_at | TIMESTAMP | CuÃ¡ndo se guardÃ³ en BD |

## ğŸ”§ ConfiguraciÃ³n Avanzada

### Modificar Horarios de Scraping
Edita `scheduler.py`:
```python
# Ejecutar cada 2 horas
schedule.every(2).hours.do(run_scraping_job)

# Ejecutar solo en dÃ­as laborables
schedule.every().monday.at("09:00").do(run_scraping_job)
```

### Ajustar Delays entre Requests
Edita `settings.py`:
```python
DOWNLOAD_DELAY = 2  # 2 segundos entre requests
RANDOMIZE_DOWNLOAD_DELAY = 0.5  # Randomizar Â±50%
```

## ğŸ“Š Monitoreo y Consultas

### Ver Logs
```bash
# Logs del programador
tail -f scraping_scheduler.log

# Logs de Scrapy
scrapy crawl losandes -L INFO
```

### Consultar Base de Datos
```sql
-- Ver todas las noticias
SELECT * FROM noticias ORDER BY created_at DESC;

-- Contar por fuente
SELECT fuente, COUNT(*) FROM noticias GROUP BY fuente;

-- Noticias de hoy
SELECT * FROM noticias WHERE DATE(created_at) = CURRENT_DATE;

-- Buscar por categorÃ­a
SELECT titulo, fecha FROM noticias WHERE categoria = 'Deportes';
```

## ğŸš¨ SoluciÃ³n de Problemas

### Error de ConexiÃ³n a PostgreSQL
1. Verifica que PostgreSQL estÃ© ejecutÃ¡ndose
2. Revisa las credenciales en `.env`
3. AsegÃºrate de que la base de datos `noticias` existe

### Spiders No Encuentran Datos
1. Verifica que los selectores CSS sean correctos
2. Revisa si las pÃ¡ginas han cambiado su estructura
3. Aumenta el delay entre requests

### Error de Permisos
```bash
# En Windows, ejecutar como administrador
# En Linux/Mac
chmod +x *.py
```

## ğŸ”„ Flujo de Trabajo

1. **ConfiguraciÃ³n Inicial**: `python install_and_setup.py`
2. **Primera EjecuciÃ³n**: `python run_scraping.py`
3. **Verificar Datos**: Consultar la base de datos
4. **Configurar AutomatizaciÃ³n**: `python scheduler.py`
5. **Monitoreo**: Revisar logs y datos periÃ³dicamente

## ğŸ“ˆ CaracterÃ­sticas del Sistema

- âœ… **DetecciÃ³n de Duplicados**: Por URL Ãºnica
- âœ… **Limpieza AutomÃ¡tica**: Fechas, contenido, estadÃ­sticas
- âœ… **Logging Detallado**: Para monitoreo y debugging
- âœ… **ConfiguraciÃ³n Flexible**: Delays, horarios, selectores
- âœ… **MÃºltiples Formatos**: PostgreSQL + JSON + CSV
- âœ… **Escalable**: FÃ¡cil agregar nuevas fuentes

## ğŸ¯ PrÃ³ximos Pasos Sugeridos

1. **Convertir Spiders de Colab**: Usar `convert_colab_to_scrapy.py`
2. **Ajustar Selectores**: Revisar cada spider segÃºn el sitio
3. **Configurar Horarios**: Ajustar segÃºn tus necesidades
4. **Monitorear Rendimiento**: Revisar logs regularmente
5. **Agregar Nuevas Fuentes**: Crear spiders adicionales

## ğŸ“ Soporte

Si tienes problemas:
1. Revisa los logs en `scraping_scheduler.log`
2. Ejecuta `python test_scraping.py` para diagnÃ³stico
3. Verifica la conexiÃ³n a PostgreSQL
4. Revisa que los spiders tengan los selectores correctos

Â¡Tu sistema de scraping estÃ¡ listo para funcionar! ğŸ‰
