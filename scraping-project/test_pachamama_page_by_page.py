#!/usr/bin/env python3
"""
Prueba del sistema p√°gina por p√°gina con Pachamama Radio
"""

import os
import sys
import time
from datetime import datetime

# Agregar el directorio actual al path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

def test_pachamama_page_by_page():
    """Probar Pachamama Radio p√°gina por p√°gina"""
    print("üß™ PRUEBA: PACHAMAMA RADIO P√ÅGINA POR P√ÅGINA")
    print("=" * 60)
    print(f"‚è∞ Inicio: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)
    
    try:
        from migrate_pachamamaradio_to_db import migrate_from_csv
        from spiders.pachamamaradio_local import PachamamaRadioLocalScraper
        
        spider = PachamamaRadioLocalScraper()
        total_articles = 0
        page_count = 0
        
        # Obtener todas las p√°ginas
        pages = spider.get_archive_pages()
        print(f"üìÑ Total de p√°ginas a procesar: {len(pages)}")
        
        # Procesar solo las primeras 3 p√°ginas para la prueba
        max_pages = min(3, len(pages))
        print(f"üéØ Procesando solo las primeras {max_pages} p√°ginas para la prueba")
        
        for i in range(max_pages):
            page_url = pages[i]
            page_count += 1
            print(f"\nüìÑ Procesando p√°gina {page_count}/{max_pages}: {page_url}")
            
            try:
                # Extraer datos de la p√°gina
                articles = spider.scrape_page(page_url)
                print(f"   üì∞ Art√≠culos encontrados: {len(articles)}")
                
                if articles:
                    # Crear archivos temporales
                    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                    temp_csv = f"data/pachamamaradio/temp_pachamama_{timestamp}.csv"
                    temp_json = f"data/pachamamaradio/temp_pachamama_{timestamp}.json"
                    
                    # Asegurar que el directorio existe
                    os.makedirs(os.path.dirname(temp_csv), exist_ok=True)
                    
                    # Guardar datos temporalmente
                    spider.save_to_files(articles, temp_csv, temp_json)
                    
                    # Migrar inmediatamente
                    print(f"   üóÑÔ∏è  Migrando {len(articles)} art√≠culos...")
                    migrated_count = migrate_from_csv(temp_csv)
                    print(f"   ‚úÖ Migrados: {migrated_count} art√≠culos")
                    
                    total_articles += migrated_count
                    
                    # Limpiar archivos temporales
                    if os.path.exists(temp_csv):
                        os.remove(temp_csv)
                    if os.path.exists(temp_json):
                        os.remove(temp_json)
                else:
                    print("   ‚ö†Ô∏è  No se encontraron art√≠culos en esta p√°gina")
                
                # Peque√±a pausa entre p√°ginas
                time.sleep(2)
                
            except Exception as e:
                print(f"   ‚ùå Error en p√°gina {page_count}: {e}")
                continue
        
        print(f"\n‚úÖ Prueba completada: {total_articles} art√≠culos migrados")
        return total_articles
        
    except Exception as e:
        print(f"‚ùå Error en la prueba: {e}")
        import traceback
        traceback.print_exc()
        return 0

if __name__ == "__main__":
    test_pachamama_page_by_page()
