# ğŸ“° Sistema de Scraping de Noticias

Este proyecto realiza scraping automÃ¡tico de noticias de diferentes fuentes y las almacena en una base de datos PostgreSQL.

## ğŸš€ CaracterÃ­sticas

- **Scraping automÃ¡tico** de mÃºltiples fuentes de noticias
- **Almacenamiento en PostgreSQL** con detecciÃ³n de duplicados
- **ProgramaciÃ³n de tareas** para ejecuciÃ³n automÃ¡tica
- **Pipeline de limpieza** de datos
- **Logging detallado** para monitoreo

## ğŸ“‹ Fuentes de Noticias

- Los Andes
- Pachamama Radio
- Puno Noticias
- Sin Fronteras

## ğŸ—„ï¸ Estructura de la Base de Datos

La tabla `noticias` contiene los siguientes campos:

| Campo | Tipo | DescripciÃ³n |
|-------|------|-------------|
| id | SERIAL | Clave primaria |
| titulo | TEXT | TÃ­tulo de la noticia |
| fecha | TIMESTAMP | Fecha de publicaciÃ³n |
| resumen | TEXT | Resumen de la noticia |
| contenido | TEXT | Contenido completo |
| categoria | VARCHAR(100) | CategorÃ­a de la noticia |
| autor | VARCHAR(200) | Autor de la noticia |
| tags | TEXT | Tags asociados |
| url | TEXT | URL Ãºnica de la noticia |
| fecha_extraccion | TIMESTAMP | Fecha de extracciÃ³n |
| caracteres_contenido | INTEGER | NÃºmero de caracteres |
| palabras_contenido | INTEGER | NÃºmero de palabras |
| imagenes | TEXT | URLs de imÃ¡genes |
| fuente | VARCHAR(100) | Fuente de la noticia |
| created_at | TIMESTAMP | Fecha de creaciÃ³n en BD |

## ğŸ› ï¸ InstalaciÃ³n

### 1. Instalar dependencias

```bash
pip install -r requeriments.txt
```

### 2. Configurar PostgreSQL

1. Instalar PostgreSQL en tu sistema
2. Crear la base de datos:

```bash
python setup_database.py
```

### 3. Configurar variables de entorno

Copia el archivo `env_example.txt` a `.env` y modifica las credenciales:

```bash
cp env_example.txt .env
```

Edita el archivo `.env` con tus credenciales de PostgreSQL:

```
DB_HOST=localhost
DB_NAME=noticias
DB_USER=postgres
DB_PASSWORD=tu_contraseÃ±a
DB_PORT=5432
```

## ğŸš€ Uso

### Ejecutar scraping manual

```bash
# Ejecutar todos los spiders
python run_scraping.py

# O ejecutar un spider especÃ­fico
scrapy crawl losandes
scrapy crawl pachamamaradio
scrapy crawl punonoticias
scrapy crawl sinfronteras
```

### Ejecutar con programador automÃ¡tico

```bash
python scheduler.py
```

El programador ejecutarÃ¡ el scraping:
- Cada 6 horas
- Diario a las 06:00, 12:00 y 18:00

## ğŸ“ Estructura del Proyecto

```
scraping-project/
â”œâ”€â”€ spiders/                 # Spiders de Scrapy
â”‚   â”œâ”€â”€ base_spider.py      # Spider base
â”‚   â”œâ”€â”€ losandes_spider.py  # Spider Los Andes
â”‚   â”œâ”€â”€ pachamamaradio_spider.py
â”‚   â”œâ”€â”€ punonoticias_spider.py
â”‚   â””â”€â”€ sinfronteras_spider.py
â”œâ”€â”€ pepelines/              # Pipelines de procesamiento
â”‚   â”œâ”€â”€ clean_pipeline.py   # Limpieza de datos
â”‚   â””â”€â”€ postgres_pipeline.py # Guardado en PostgreSQL
â”œâ”€â”€ config/                 # Configuraciones
â”‚   â””â”€â”€ database.py         # Config de BD
â”œâ”€â”€ data/                   # Datos exportados
â”œâ”€â”€ run_scraping.py         # Script principal
â”œâ”€â”€ scheduler.py            # Programador de tareas
â”œâ”€â”€ setup_database.py       # ConfiguraciÃ³n de BD
â””â”€â”€ settings.py             # ConfiguraciÃ³n de Scrapy
```

## ğŸ”§ ConfiguraciÃ³n Avanzada

### Modificar horarios de scraping

Edita el archivo `scheduler.py` para cambiar los horarios:

```python
# Ejecutar cada 2 horas
schedule.every(2).hours.do(run_scraping_job)

# Ejecutar solo en dÃ­as laborables
schedule.every().monday.at("09:00").do(run_scraping_job)
```

### Configurar delays entre requests

Modifica `settings.py`:

```python
DOWNLOAD_DELAY = 2  # 2 segundos entre requests
RANDOMIZE_DOWNLOAD_DELAY = 0.5  # Randomizar Â±50%
```

## ğŸ“Š Monitoreo

### Ver logs

```bash
# Logs del programador
tail -f scraping_scheduler.log

# Logs de Scrapy
scrapy crawl losandes -L INFO
```

### Consultar base de datos

```sql
-- Ver todas las noticias
SELECT * FROM noticias ORDER BY created_at DESC;

-- Contar noticias por fuente
SELECT fuente, COUNT(*) FROM noticias GROUP BY fuente;

-- Noticias de hoy
SELECT * FROM noticias WHERE DATE(created_at) = CURRENT_DATE;
```

## ğŸš¨ SoluciÃ³n de Problemas

### Error de conexiÃ³n a PostgreSQL

1. Verifica que PostgreSQL estÃ© ejecutÃ¡ndose
2. Revisa las credenciales en `.env`
3. AsegÃºrate de que la base de datos `noticias` existe

### Error de permisos

```bash
# En Windows, ejecutar como administrador
# En Linux/Mac, verificar permisos de archivos
chmod +x *.py
```

### Spiders no encuentran datos

1. Verifica que los selectores CSS/XPath sean correctos
2. Revisa si las pÃ¡ginas han cambiado su estructura
3. Aumenta el delay entre requests

## ğŸ“ Notas Importantes

- El sistema detecta automÃ¡ticamente noticias duplicadas por URL
- Los datos se guardan tanto en PostgreSQL como en archivos JSON/CSV
- El scraping es respetuoso con delays configurados
- Se mantiene un log detallado de todas las operaciones

## ğŸ¤ Contribuir

1. Fork el proyecto
2. Crea una rama para tu feature
3. Commit tus cambios
4. Push a la rama
5. Abre un Pull Request

## ğŸ“„ Licencia

Este proyecto es de uso educativo y de investigaciÃ³n.
