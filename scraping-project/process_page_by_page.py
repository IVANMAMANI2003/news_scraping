#!/usr/bin/env python3
"""
Sistema de procesamiento p√°gina por p√°gina:
1. Extrae datos de una p√°gina
2. Migra inmediatamente a la base de datos
3. Contin√∫a con la siguiente p√°gina
"""

import os
import sys
import time
from datetime import datetime

# Agregar el directorio actual al path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

def process_losandes_page_by_page():
    """Procesar Los Andes p√°gina por p√°gina"""
    print("üï∑Ô∏è  LOS ANDES - PROCESAMIENTO P√ÅGINA POR P√ÅGINA")
    print("=" * 60)
    
    try:
        from migrate_losandes_to_db import migrate_from_csv, migrate_from_json
        from spiders.losandes_local import LosAndesSpider
        
        spider = LosAndesSpider()
        total_articles = 0
        page_count = 0
        
        # Obtener todas las p√°ginas
        pages = spider.get_archive_pages()
        print(f"üìÑ Total de p√°ginas a procesar: {len(pages)}")
        
        for page_url in pages:
            page_count += 1
            print(f"\nüìÑ Procesando p√°gina {page_count}/{len(pages)}: {page_url}")
            
            try:
                # Extraer datos de la p√°gina
                articles = spider.scrape_page(page_url)
                print(f"   üì∞ Art√≠culos encontrados: {len(articles)}")
                
                if articles:
                    # Crear archivos temporales
                    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                    temp_csv = f"data/losandes/temp_losandes_{timestamp}.csv"
                    temp_json = f"data/losandes/temp_losandes_{timestamp}.json"
                    
                    # Asegurar que el directorio existe
                    os.makedirs(os.path.dirname(temp_csv), exist_ok=True)
                    
                    # Guardar datos temporalmente
                    spider.save_to_files(articles, temp_csv, temp_json)
                    
                    # Migrar inmediatamente
                    print(f"   üóÑÔ∏è  Migrando {len(articles)} art√≠culos...")
                    migrated_count = migrate_from_csv(temp_csv)
                    print(f"   ‚úÖ Migrados: {migrated_count} art√≠culos")
                    
                    total_articles += migrated_count
                    
                    # Limpiar archivos temporales
                    if os.path.exists(temp_csv):
                        os.remove(temp_csv)
                    if os.path.exists(temp_json):
                        os.remove(temp_json)
                else:
                    print("   ‚ö†Ô∏è  No se encontraron art√≠culos en esta p√°gina")
                
                # Peque√±a pausa entre p√°ginas
                time.sleep(1)
                
            except Exception as e:
                print(f"   ‚ùå Error en p√°gina {page_count}: {e}")
                continue
        
        print(f"\n‚úÖ Los Andes completado: {total_articles} art√≠culos migrados")
        return total_articles
        
    except Exception as e:
        print(f"‚ùå Error en Los Andes: {e}")
        return 0

def process_punonoticias_page_by_page():
    """Procesar Puno Noticias p√°gina por p√°gina"""
    print("\nüï∑Ô∏è  PUNO NOTICIAS - PROCESAMIENTO P√ÅGINA POR P√ÅGINA")
    print("=" * 60)
    
    try:
        from migrate_punonoticias_to_db import (migrate_from_csv,
                                                migrate_from_json)
        from spiders.punonoticias_local import PunoNoticiasSpider
        
        spider = PunoNoticiasSpider()
        total_articles = 0
        page_count = 0
        
        # Obtener todas las p√°ginas
        pages = spider.get_archive_pages()
        print(f"üìÑ Total de p√°ginas a procesar: {len(pages)}")
        
        for page_url in pages:
            page_count += 1
            print(f"\nüìÑ Procesando p√°gina {page_count}/{len(pages)}: {page_url}")
            
            try:
                # Extraer datos de la p√°gina
                articles = spider.scrape_page(page_url)
                print(f"   üì∞ Art√≠culos encontrados: {len(articles)}")
                
                if articles:
                    # Crear archivos temporales
                    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                    temp_csv = f"data/punonoticias/temp_punonoticias_{timestamp}.csv"
                    temp_json = f"data/punonoticias/temp_punonoticias_{timestamp}.json"
                    
                    # Asegurar que el directorio existe
                    os.makedirs(os.path.dirname(temp_csv), exist_ok=True)
                    
                    # Guardar datos temporalmente
                    spider.save_to_files(articles, temp_csv, temp_json)
                    
                    # Migrar inmediatamente
                    print(f"   üóÑÔ∏è  Migrando {len(articles)} art√≠culos...")
                    migrated_count = migrate_from_csv(temp_csv)
                    print(f"   ‚úÖ Migrados: {migrated_count} art√≠culos")
                    
                    total_articles += migrated_count
                    
                    # Limpiar archivos temporales
                    if os.path.exists(temp_csv):
                        os.remove(temp_csv)
                    if os.path.exists(temp_json):
                        os.remove(temp_json)
                else:
                    print("   ‚ö†Ô∏è  No se encontraron art√≠culos en esta p√°gina")
                
                # Peque√±a pausa entre p√°ginas
                time.sleep(1)
                
            except Exception as e:
                print(f"   ‚ùå Error en p√°gina {page_count}: {e}")
                continue
        
        print(f"\n‚úÖ Puno Noticias completado: {total_articles} art√≠culos migrados")
        return total_articles
        
    except Exception as e:
        print(f"‚ùå Error en Puno Noticias: {e}")
        return 0

def process_pachamama_page_by_page():
    """Procesar Pachamama Radio p√°gina por p√°gina"""
    print("\nüï∑Ô∏è  PACHAMAMA RADIO - PROCESAMIENTO P√ÅGINA POR P√ÅGINA")
    print("=" * 60)
    
    try:
        from migrate_pachamamaradio_to_db import (migrate_from_csv,
                                                  migrate_from_json)
        from spiders.pachamamaradio_local import PachamamaRadioSpider
        
        spider = PachamamaRadioSpider()
        total_articles = 0
        page_count = 0
        
        # Obtener todas las p√°ginas
        pages = spider.get_archive_pages()
        print(f"üìÑ Total de p√°ginas a procesar: {len(pages)}")
        
        for page_url in pages:
            page_count += 1
            print(f"\nüìÑ Procesando p√°gina {page_count}/{len(pages)}: {page_url}")
            
            try:
                # Extraer datos de la p√°gina
                articles = spider.scrape_page(page_url)
                print(f"   üì∞ Art√≠culos encontrados: {len(articles)}")
                
                if articles:
                    # Crear archivos temporales
                    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                    temp_csv = f"data/pachamamaradio/temp_pachamama_{timestamp}.csv"
                    temp_json = f"data/pachamamaradio/temp_pachamama_{timestamp}.json"
                    
                    # Asegurar que el directorio existe
                    os.makedirs(os.path.dirname(temp_csv), exist_ok=True)
                    
                    # Guardar datos temporalmente
                    spider.save_to_files(articles, temp_csv, temp_json)
                    
                    # Migrar inmediatamente
                    print(f"   üóÑÔ∏è  Migrando {len(articles)} art√≠culos...")
                    migrated_count = migrate_from_csv(temp_csv)
                    print(f"   ‚úÖ Migrados: {migrated_count} art√≠culos")
                    
                    total_articles += migrated_count
                    
                    # Limpiar archivos temporales
                    if os.path.exists(temp_csv):
                        os.remove(temp_csv)
                    if os.path.exists(temp_json):
                        os.remove(temp_json)
                else:
                    print("   ‚ö†Ô∏è  No se encontraron art√≠culos en esta p√°gina")
                
                # Peque√±a pausa entre p√°ginas
                time.sleep(1)
                
            except Exception as e:
                print(f"   ‚ùå Error en p√°gina {page_count}: {e}")
                continue
        
        print(f"\n‚úÖ Pachamama Radio completado: {total_articles} art√≠culos migrados")
        return total_articles
        
    except Exception as e:
        print(f"‚ùå Error en Pachamama Radio: {e}")
        return 0

def process_sinfronteras_page_by_page():
    """Procesar Sin Fronteras p√°gina por p√°gina"""
    print("\nüï∑Ô∏è  SIN FRONTERAS - PROCESAMIENTO P√ÅGINA POR P√ÅGINA")
    print("=" * 60)
    
    try:
        from migrate_sinfronteras_to_db import (migrate_from_csv,
                                                migrate_from_json)
        from spiders.sinfronteras_local import SinFronterasSpider
        
        spider = SinFronterasSpider()
        total_articles = 0
        page_count = 0
        
        # Obtener todas las p√°ginas
        pages = spider.get_archive_pages()
        print(f"üìÑ Total de p√°ginas a procesar: {len(pages)}")
        
        for page_url in pages:
            page_count += 1
            print(f"\nüìÑ Procesando p√°gina {page_count}/{len(pages)}: {page_url}")
            
            try:
                # Extraer datos de la p√°gina
                articles = spider.scrape_page(page_url)
                print(f"   üì∞ Art√≠culos encontrados: {len(articles)}")
                
                if articles:
                    # Crear archivos temporales
                    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                    temp_csv = f"data/sinfronteras/temp_sinfronteras_{timestamp}.csv"
                    temp_json = f"data/sinfronteras/temp_sinfronteras_{timestamp}.json"
                    
                    # Asegurar que el directorio existe
                    os.makedirs(os.path.dirname(temp_csv), exist_ok=True)
                    
                    # Guardar datos temporalmente
                    spider.save_to_files(articles, temp_csv, temp_json)
                    
                    # Migrar inmediatamente
                    print(f"   üóÑÔ∏è  Migrando {len(articles)} art√≠culos...")
                    migrated_count = migrate_from_csv(temp_csv)
                    print(f"   ‚úÖ Migrados: {migrated_count} art√≠culos")
                    
                    total_articles += migrated_count
                    
                    # Limpiar archivos temporales
                    if os.path.exists(temp_csv):
                        os.remove(temp_csv)
                    if os.path.exists(temp_json):
                        os.remove(temp_json)
                else:
                    print("   ‚ö†Ô∏è  No se encontraron art√≠culos en esta p√°gina")
                
                # Peque√±a pausa entre p√°ginas
                time.sleep(1)
                
            except Exception as e:
                print(f"   ‚ùå Error en p√°gina {page_count}: {e}")
                continue
        
        print(f"\n‚úÖ Sin Fronteras completado: {total_articles} art√≠culos migrados")
        return total_articles
        
    except Exception as e:
        print(f"‚ùå Error en Sin Fronteras: {e}")
        return 0

def main():
    """Funci√≥n principal"""
    print("üöÄ PROCESAMIENTO P√ÅGINA POR P√ÅGINA")
    print("=" * 80)
    print(f"‚è∞ Inicio: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80)
    print("üéØ Objetivo: Extraer ‚Üí Migrar ‚Üí Continuar")
    print("üìä Fuentes: Los Andes, Puno Noticias, Pachamama Radio, Sin Fronteras")
    print("=" * 80)
    
    total_articles = 0
    
    # Procesar cada fuente
    total_articles += process_losandes_page_by_page()
    total_articles += process_punonoticias_page_by_page()
    total_articles += process_pachamama_page_by_page()
    total_articles += process_sinfronteras_page_by_page()
    
    # Resumen final
    print(f"\n" + "=" * 80)
    print(f"üéâ ¬°PROCESAMIENTO COMPLETADO!")
    print(f"‚è∞ Finalizado: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"üìä Total de art√≠culos migrados: {total_articles}")
    print(f"üóÑÔ∏è  Datos guardados en: PostgreSQL (tabla 'noticias')")
    print(f"=" * 80)

if __name__ == "__main__":
    main()
