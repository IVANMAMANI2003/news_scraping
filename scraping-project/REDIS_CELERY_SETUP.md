# ğŸš€ ConfiguraciÃ³n de Redis y Celery para Scraping Automatizado

## ğŸ“‹ DescripciÃ³n

Este proyecto integra Redis como broker de tareas y Celery para automatizar el scraping de noticias. El sistema ejecuta tareas programadas cada 6 horas, evita duplicados y guarda datos directamente en PostgreSQL.

## ğŸ—ï¸ Arquitectura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Redis Broker  â”‚    â”‚  Celery Workers â”‚    â”‚   PostgreSQL    â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚  - Cola scrapingâ”‚â—„â”€â”€â–ºâ”‚  - Worker 1     â”‚â—„â”€â”€â–ºâ”‚  - Tabla noticiasâ”‚
â”‚  - Cola migrationâ”‚    â”‚  - Worker 2     â”‚    â”‚  - DetecciÃ³n    â”‚
â”‚  - Cola cleanup â”‚    â”‚  - Worker 3     â”‚    â”‚    duplicados   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â–²                       â–²
         â”‚                       â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
    â”‚ Celery  â”‚            â”‚  Spiders  â”‚
    â”‚  Beat   â”‚            â”‚           â”‚
    â”‚(Scheduler)â”‚          â”‚ - Los Andesâ”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚ - Puno    â”‚
                           â”‚ - Pachamamaâ”‚
                           â”‚ - Sin Fronterasâ”‚
                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Estructura de Archivos

```
proyecto/
â”œâ”€â”€ celery_app.py                 # ConfiguraciÃ³n principal de Celery
â”œâ”€â”€ config/
â”‚   â””â”€â”€ redis_config.py          # ConfiguraciÃ³n de Redis y Celery
â”œâ”€â”€ celery_tasks/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ scraping_tasks.py        # Tareas de scraping
â”‚   â”œâ”€â”€ migration_tasks.py       # Tareas de migraciÃ³n
â”‚   â””â”€â”€ cleanup_tasks.py         # Tareas de limpieza
â”œâ”€â”€ celery_workers/
â”‚   â”œâ”€â”€ start_worker.py          # Script para iniciar workers
â”‚   â”œâ”€â”€ start_beat.py            # Script para iniciar scheduler
â”‚   â””â”€â”€ control_tasks.py         # Script de control de tareas
â”œâ”€â”€ start_redis_celery.py        # Iniciador automÃ¡tico
â”œâ”€â”€ install_redis_celery.py      # Instalador
â””â”€â”€ docker-compose-redis.yml     # Docker para Redis y PostgreSQL
```

## ğŸš€ InstalaciÃ³n

### 1. InstalaciÃ³n AutomÃ¡tica

```bash
python install_redis_celery.py
```

### 2. InstalaciÃ³n Manual

#### Instalar dependencias:
```bash
pip install -r api/requirements.txt
```

#### Instalar Redis:
- **Windows**: Descargar desde [GitHub](https://github.com/microsoftarchive/redis/releases)
- **Linux**: `sudo apt-get install redis-server`
- **macOS**: `brew install redis`
- **Docker**: `docker run -d -p 6379:6379 redis:alpine`

#### Instalar PostgreSQL:
- Usar Docker: `docker-compose -f docker-compose-redis.yml up -d`

## ğŸ¯ Uso

### 1. Iniciar Servicios

#### OpciÃ³n A: AutomÃ¡tico
```bash
python start_redis_celery.py
```

#### OpciÃ³n B: Manual
```bash
# Terminal 1: Redis
redis-server

# Terminal 2: Worker de scraping
python celery_workers/start_worker.py --queue scraping

# Terminal 3: Worker de migraciÃ³n
python celery_workers/start_worker.py --queue migration

# Terminal 4: Worker de limpieza
python celery_workers/start_worker.py --queue cleanup

# Terminal 5: Scheduler
python celery_workers/start_beat.py
```

### 2. Control de Tareas

#### Ver estado de colas:
```bash
python celery_workers/control_tasks.py list-queues
```

#### Disparar scraping inmediato:
```bash
python celery_workers/control_tasks.py trigger-scraping
```

#### Disparar migraciÃ³n:
```bash
python celery_workers/control_tasks.py trigger-migration
```

#### Ver resultado de tarea:
```bash
python celery_workers/control_tasks.py get-result --task-id <ID>
```

#### Limpiar colas:
```bash
python celery_workers/control_tasks.py purge
```

## âš™ï¸ ConfiguraciÃ³n

### Variables de Entorno (.env)

```env
# Redis
REDIS_URL=redis://localhost:6379/0

# Base de Datos
DATABASE_URL=postgresql://postgres:123456@localhost:5432/noticias

# Scraping
SCRAPING_INTERVAL_HOURS=6
DUPLICATE_CHECK_ENABLED=true

# Logging
LOG_LEVEL=INFO
```

### ConfiguraciÃ³n de Tareas Programadas

En `celery_app.py`:

```python
celery_app.conf.beat_schedule = {
    'scrape-all-sources': {
        'task': 'celery_tasks.scraping_tasks.scrape_all_sources',
        'schedule': 21600.0,  # 6 horas
    },
    # Limpieza automÃ¡tica deshabilitada para mantener mÃ¡ximo historial de datos
    # 'cleanup-old-data': {
    #     'task': 'celery_tasks.cleanup_tasks.cleanup_old_data',
    #     'schedule': 86400.0,  # 24 horas
    # },
}
```

## ğŸ”§ Tareas Disponibles

### Scraping Tasks
- `scrape_all_sources`: Scraping de todas las fuentes
- `scrape_single_source`: Scraping de una fuente especÃ­fica
- `scrape_source_now`: Scraping inmediato
- `check_duplicates`: Verificar duplicados

### Migration Tasks
- `migrate_source_to_db`: Migrar una fuente a PostgreSQL
- `migrate_all_sources`: Migrar todas las fuentes

### Cleanup Tasks
- `cleanup_old_data`: Limpiar datos antiguos
- `cleanup_duplicates`: Eliminar duplicados
- `optimize_database`: Optimizar base de datos
- `database_health_check`: Verificar salud de la BD

## ğŸ“Š Monitoreo

### Ver tareas activas:
```bash
python celery_workers/control_tasks.py list-active
```

### Ver tareas programadas:
```bash
python celery_workers/control_tasks.py list-scheduled
```

### Verificar salud de la base de datos:
```python
from celery_tasks.cleanup_tasks import database_health_check
result = database_health_check.delay()
print(result.get())
```

## ğŸ› SoluciÃ³n de Problemas

### Redis no conecta:
1. Verificar que Redis estÃ© ejecutÃ¡ndose: `redis-cli ping`
2. Verificar puerto: `netstat -an | grep 6379`
3. Reiniciar Redis: `redis-server`

### Workers no procesan tareas:
1. Verificar conexiÃ³n a Redis
2. Verificar configuraciÃ³n de colas
3. Revisar logs de workers

### Duplicados en la base de datos:
1. Ejecutar limpieza: `python celery_workers/control_tasks.py trigger-cleanup`
2. Verificar configuraciÃ³n de detecciÃ³n de duplicados

### Tareas fallan:
1. Revisar logs de workers
2. Verificar conectividad a sitios web
3. Verificar configuraciÃ³n de base de datos

## ğŸ“ˆ Escalabilidad

### Aumentar workers:
```bash
# MÃºltiples workers para scraping
python celery_workers/start_worker.py --queue scraping --concurrency 8

# Workers especializados
python celery_workers/start_worker.py --queue migration --concurrency 2
```

### Configurar Redis Cluster:
```python
# En redis_config.py
REDIS_URL = 'redis://node1:6379,redis://node2:6379,redis://node3:6379'
```

### Usar Flower para monitoreo:
```bash
pip install flower
celery -A celery_app flower
```

## ğŸ”’ Seguridad

### Configurar autenticaciÃ³n Redis:
```python
REDIS_URL = 'redis://:password@localhost:6379/0'
```

### Configurar SSL:
```python
REDIS_URL = 'rediss://localhost:6380/0'
```

### Restringir acceso a base de datos:
```python
DATABASE_URL = 'postgresql://user:pass@localhost:5432/noticias?sslmode=require'
```

## ğŸ“ Logs

Los logs se guardan en:
- Workers: Salida estÃ¡ndar
- Celery Beat: Salida estÃ¡ndar
- AplicaciÃ³n: `logs/` (configurable)

Configurar logging en `celery_app.py`:
```python
import logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/celery.log'),
        logging.StreamHandler()
    ]
)
```

## ğŸ”§ **Configuraciones Especiales**

### ğŸ“Š **ConservaciÃ³n de Datos**
- **Sin limpieza automÃ¡tica**: Los datos se conservan indefinidamente para mantener el mÃ¡ximo historial
- **Base de datos creciente**: Acumula todas las noticias extraÃ­das sin eliminaciÃ³n automÃ¡tica
- **Limpieza manual**: Disponible solo cuando sea necesario ejecutar `cleanup_old_data`

### ğŸ–¼ï¸ **LimitaciÃ³n de ImÃ¡genes**
- **MÃ¡ximo 2 imÃ¡genes** por noticia
- **Filtrado inteligente**: Excluye iconos, logos, avatares y botones
- **Filtrado por tamaÃ±o**: Excluye imÃ¡genes menores a 100x100 pÃ­xeles
- **OptimizaciÃ³n de almacenamiento**: Reduce el tamaÃ±o de los datos guardados

## ğŸ‰ Â¡Listo!

Tu sistema de scraping automatizado estÃ¡ configurado. Las noticias se extraerÃ¡n automÃ¡ticamente cada 6 horas y se guardarÃ¡n en PostgreSQL sin duplicados, conservando todo el historial de datos.
