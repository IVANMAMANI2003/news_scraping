#!/usr/bin/env python3
"""
Script de prueba simple para Puno Noticias
"""

import json
from datetime import datetime

import requests
from bs4 import BeautifulSoup


def test_punonoticias_connection():
    """Probar conexiÃ³n a Puno Noticias"""
    print("ğŸ” Probando conexiÃ³n a Puno Noticias...")
    
    try:
        url = "https://punonoticias.pe"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        print(f"âœ… ConexiÃ³n exitosa a {url}")
        print(f"ğŸ“Š Status Code: {response.status_code}")
        print(f"ğŸ“Š TamaÃ±o de respuesta: {len(response.content)} bytes")
        
        # Probar parsing bÃ¡sico
        soup = BeautifulSoup(response.content, 'html.parser')
        title = soup.find('title')
        
        if title:
            print(f"ğŸ“„ TÃ­tulo de la pÃ¡gina: {title.get_text()}")
        
        # Buscar algunos enlaces
        links = soup.find_all('a', href=True)[:5]
        print(f"ğŸ”— Primeros 5 enlaces encontrados:")
        for i, link in enumerate(links, 1):
            href = link.get('href')
            text = link.get_text().strip()[:50]
            print(f"   {i}. {text}... -> {href}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Error de conexiÃ³n: {e}")
        return False

def test_simple_scraping():
    """Probar scraping simple de un artÃ­culo"""
    print(f"\nğŸ•·ï¸  Probando scraping simple...")
    
    try:
        # URL de ejemplo (puedes cambiarla por una real)
        test_url = "https://punonoticias.pe"
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        response = requests.get(test_url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Buscar tÃ­tulos de noticias
        titles = soup.find_all(['h1', 'h2', 'h3'])[:3]
        
        print(f"ğŸ“° TÃ­tulos encontrados:")
        for i, title in enumerate(titles, 1):
            text = title.get_text().strip()
            if text:
                print(f"   {i}. {text}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Error en scraping: {e}")
        return False

def main():
    """FunciÃ³n principal"""
    print("ğŸ§ª Prueba Simple de Puno Noticias")
    print("=" * 50)
    
    # Probar conexiÃ³n
    connection_ok = test_punonoticias_connection()
    
    if connection_ok:
        # Probar scraping bÃ¡sico
        scraping_ok = test_simple_scraping()
        
        if scraping_ok:
            print(f"\nğŸ‰ Â¡Todo funcionando correctamente!")
            print(f"âœ… Puedes ejecutar el scraping completo:")
            print(f"   python spiders/punonoticias_local.py")
        else:
            print(f"\nâš ï¸  Hay problemas con el scraping")
    else:
        print(f"\nâŒ Hay problemas de conexiÃ³n")
        print(f"ğŸ’¡ Verifica tu conexiÃ³n a internet")

if __name__ == "__main__":
    main()
